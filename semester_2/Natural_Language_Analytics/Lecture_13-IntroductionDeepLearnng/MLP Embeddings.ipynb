{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-layer Perceptron (with Embeddings)\n",
    "\n",
    "This notebook is an example of a multi-layer perceptron with Keras (https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petasis/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "# Import some needed packages\n",
    "import random\n",
    "random.seed(2018)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We are going to load the tweets from SemEval 2018..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*train-*.tsv'\n",
    "filenames = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "# print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all files into a big data frame...\n",
    "column_names = ['id', 'tag', 'tweet']\n",
    "df = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in filenames], ignore_index=True, sort=True)\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows having 'Not Available'...\n",
    "df = df[df.tweet != 'Not Available']\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to convert a tweet into a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# turn a document into a list of clean tokens\n",
    "def clean_doc(doc):\n",
    "    # Remove links...\n",
    "    doc = re.sub(\"\\w+:\\/\\/\\S+\", \" \", doc)\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all tweets, and save results in the dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['tokens'] = np.array([ clean_doc(tweet) for tweet in df.tweet ])\n",
    "# df.info()  \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform all actions also for dev/test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*dev-*.tsv'\n",
    "devfs    = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*test-*.tsv'\n",
    "testfs   = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "df_dev   = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in devfs],  ignore_index=True, sort=True)\n",
    "df_test  = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in testfs], ignore_index=True, sort=True)\n",
    "df_dev   = df_dev[df_dev.tweet != 'Not Available']\n",
    "df_test  = df_test[df_test.tweet != 'Not Available']\n",
    "df_dev['tokens']  = np.array([ clean_doc(tweet) for tweet in df_dev.tweet ])\n",
    "df_test['tokens'] = np.array([ clean_doc(tweet) for tweet in df_test.tweet ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract our vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets:  30790\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "vocabulary = Counter()\n",
    "for tweet_tokens in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens):\n",
    "    vocabulary.update(tweet_tokens)\n",
    "\n",
    "print('Total tweets: ', sum(1 for _ in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens)))\n",
    "# vocabulary.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter words using the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_to_vector_words(tokens, vocabulary):\n",
    "    tokens = [w for w in tokens if w in vocabulary]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# print(df.tweet[0])\n",
    "# token_to_vector_words(df.tokens[0], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['vector_tokens']      = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df.tokens ])\n",
    "df_dev['vector_tokens']  = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_dev.tokens ])\n",
    "df_test['vector_tokens'] = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_test.tokens ])\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map tag from class (positive, negative) to numbers...\n",
    "df['btag']      = df.tag.astype('category').cat.codes\n",
    "df_dev['btag']  = df_dev.tag.astype('category').cat.codes\n",
    "df_test['btag'] = df_test.tag.astype('category').cat.codes\n",
    "# df_dev.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained embeddings...\n",
    "\n",
    "Pre-trained embeddings can be found:\n",
    "\n",
    "GloVe: http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Word2Vec: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings into a dict...\n",
    "embeddings_index = {}\n",
    "glove_data = '../data/embeddings/glove.twitter.27B.50d.txt'\n",
    "f = open(glove_data)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    " \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets make our vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23740, 50)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.51880002  0.039331    0.080883   ... -0.81913    -0.28933999\n",
      "   0.87558001]\n",
      " [ 0.43026     0.0081207  -0.0090224  ... -0.24276    -0.51657999\n",
      "   1.24720001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.086368    1.26409996  0.18991999 ... -0.43009001 -0.30998001\n",
      "   0.047121  ]\n",
      " [-1.40139997  1.06110001 -0.14475    ... -0.16698    -0.26725999\n",
      "  -0.70081002]]\n",
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "['Make', 'Sure', 'To', 'Come', 'To', 'The', 'Bob', 'Jones', 'Game', 'Friday', 'Free', 'Hot', 'Dogs', 'Hamburgers', 'amp', 'Food', 'outside', 'gate', 'amp', 'watch', 'Bob', 'Jones', 'take', 'Austin', 'High']\n",
      "[34, 119, 327, 29, 327, 5, 192, 1542, 15, 10, 96, 470, 1564, 14202, 11, 635, 1020, 3845, 11, 24, 192, 1542, 85, 1321, 465]\n",
      "Longest tweet (in words):  25\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.vector_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dimension = 50\n",
    "# The embedding_matrix matrix maps words to vectors in the specified embedding dimension (here 50):\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    # print(word, i) <= i starts from 1\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector;#[:embedding_dimension]\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_sequences(df.vector_tokens)\n",
    "Ytrain = df.btag\n",
    "Xtest  = tokenizer.texts_to_sequences(df_test.vector_tokens)\n",
    "Ytest  = df_test.btag\n",
    "from keras.utils import np_utils\n",
    "Ytrain_one_hot = np_utils.to_categorical(Ytrain)\n",
    "Ytest_one_hot  = np_utils.to_categorical(Ytest)\n",
    "print(Xtrain[0])\n",
    "\n",
    "## Get the longest tweet...\n",
    "longest = max(df.tokens,key=len)\n",
    "print(longest)\n",
    "longest = max(Xtrain,key=len)\n",
    "print(longest)\n",
    "longest = len(longest)\n",
    "print(\"Longest tweet (in words): \", longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to set the length of all tweets to the longest tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2625  141  358    7    6 2340 1249   49]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(Xtrain[0])\n",
    "Xtrain = pad_sequences(Xtrain, maxlen=longest)\n",
    "Xtest  = pad_sequences(Xtest,  maxlen=longest)\n",
    "print(Xtrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple MLP model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23740\n"
     ]
    }
   ],
   "source": [
    "n_words = embedding_matrix.shape[0]\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 50)            1187000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 1,190,753\n",
      "Trainable params: 3,753\n",
      "Non-trainable params: 1,187,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 199.00 264.00\" width=\"199pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 195,-260 195,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140656957004992 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140656957004992</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 191,-255.5 191,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-233.8\">embedding_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140656957004824 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140656957004824</title>\n",
       "<polygon fill=\"none\" points=\"15,-146.5 15,-182.5 176,-182.5 176,-146.5 15,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-160.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140656957004992&#45;&gt;140656957004824 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140656957004992-&gt;140656957004824</title>\n",
       "<path d=\"M95.5,-219.4551C95.5,-211.3828 95.5,-201.6764 95.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-192.5903 95.5,-182.5904 92.0001,-192.5904 99.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140656957004712 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140656957004712</title>\n",
       "<polygon fill=\"none\" points=\"41,-73.5 41,-109.5 150,-109.5 150,-73.5 41,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-87.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140656957004824&#45;&gt;140656957004712 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140656957004824-&gt;140656957004712</title>\n",
       "<path d=\"M95.5,-146.4551C95.5,-138.3828 95.5,-128.6764 95.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-119.5903 95.5,-109.5904 92.0001,-119.5904 99.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140656957004432 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140656957004432</title>\n",
       "<polygon fill=\"none\" points=\"44.5,-.5 44.5,-36.5 146.5,-36.5 146.5,-.5 44.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140656957004712&#45;&gt;140656957004432 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140656957004712-&gt;140656957004432</title>\n",
       "<path d=\"M95.5,-73.4551C95.5,-65.3828 95.5,-55.6764 95.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-46.5903 95.5,-36.5904 92.0001,-46.5904 99.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "#model.add(Dense(units=64, activation='relu', input_shape=(n_words,)))\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit our network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 2s - loss: 0.2031 - acc: 0.4903\n",
      "Epoch 2/30\n",
      " - 1s - loss: 0.1827 - acc: 0.5576\n",
      "Epoch 3/30\n",
      " - 2s - loss: 0.1752 - acc: 0.5848\n",
      "Epoch 4/30\n",
      " - 2s - loss: 0.1707 - acc: 0.5970\n",
      "Epoch 5/30\n",
      " - 1s - loss: 0.1678 - acc: 0.6085\n",
      "Epoch 6/30\n",
      " - 2s - loss: 0.1656 - acc: 0.6148\n",
      "Epoch 7/30\n",
      " - 2s - loss: 0.1640 - acc: 0.6188\n",
      "Epoch 8/30\n",
      " - 1s - loss: 0.1627 - acc: 0.6200\n",
      "Epoch 9/30\n",
      " - 2s - loss: 0.1617 - acc: 0.6252\n",
      "Epoch 10/30\n",
      " - 2s - loss: 0.1609 - acc: 0.6266\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.1599 - acc: 0.6325\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.1595 - acc: 0.6295\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.1584 - acc: 0.6367\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.1583 - acc: 0.6352\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.1575 - acc: 0.6393\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.1573 - acc: 0.6374\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.1570 - acc: 0.6397\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.1565 - acc: 0.6416\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.1562 - acc: 0.6436\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.1559 - acc: 0.6476\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.1559 - acc: 0.6447\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.1552 - acc: 0.6510\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.1552 - acc: 0.6471\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.1549 - acc: 0.6493\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.1547 - acc: 0.6486\n",
      "Epoch 26/30\n",
      " - 2s - loss: 0.1544 - acc: 0.6508\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.1545 - acc: 0.6498\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.1544 - acc: 0.6526\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.1541 - acc: 0.6545\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.1540 - acc: 0.6546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed3fb7b908>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "from keras import backend as K\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our fit network...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.906176\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add one more layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 50)            1187000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 1,190,753\n",
      "Trainable params: 3,753\n",
      "Non-trainable params: 1,187,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 199.00 264.00\" width=\"199pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 195,-260 195,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140656951170328 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140656951170328</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 191,-255.5 191,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-233.8\">embedding_2_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140656951170384 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140656951170384</title>\n",
       "<polygon fill=\"none\" points=\"15,-146.5 15,-182.5 176,-182.5 176,-146.5 15,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140656951170328&#45;&gt;140656951170384 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140656951170328-&gt;140656951170384</title>\n",
       "<path d=\"M95.5,-219.4551C95.5,-211.3828 95.5,-201.6764 95.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-192.5903 95.5,-182.5904 92.0001,-192.5904 99.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140656951170440 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140656951170440</title>\n",
       "<polygon fill=\"none\" points=\"41,-73.5 41,-109.5 150,-109.5 150,-73.5 41,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-87.8\">flatten_2: Flatten</text>\n",
       "</g>\n",
       "<!-- 140656951170384&#45;&gt;140656951170440 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140656951170384-&gt;140656951170440</title>\n",
       "<path d=\"M95.5,-146.4551C95.5,-138.3828 95.5,-128.6764 95.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-119.5903 95.5,-109.5904 92.0001,-119.5904 99.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140656951169936 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140656951169936</title>\n",
       "<polygon fill=\"none\" points=\"44.5,-.5 44.5,-36.5 146.5,-36.5 146.5,-.5 44.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140656951170440&#45;&gt;140656951169936 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140656951170440-&gt;140656951169936</title>\n",
       "<path d=\"M95.5,-73.4551C95.5,-65.3828 95.5,-55.6764 95.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.0001,-46.5903 95.5,-36.5904 92.0001,-46.5904 99.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "#model.add(Dense(units=64, activation='relu', input_shape=(n_words,)))\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=False))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 2s - loss: 0.5477 - acc: 0.7113\n",
      "Epoch 2/60\n",
      " - 2s - loss: 0.5030 - acc: 0.7438\n",
      "Epoch 3/60\n",
      " - 2s - loss: 0.4935 - acc: 0.7515\n",
      "Epoch 4/60\n",
      " - 2s - loss: 0.4890 - acc: 0.7533\n",
      "Epoch 5/60\n",
      " - 2s - loss: 0.4855 - acc: 0.7563\n",
      "Epoch 6/60\n",
      " - 2s - loss: 0.4846 - acc: 0.7582\n",
      "Epoch 7/60\n",
      " - 2s - loss: 0.4823 - acc: 0.7599\n",
      "Epoch 8/60\n",
      " - 2s - loss: 0.4815 - acc: 0.7597\n",
      "Epoch 9/60\n",
      " - 2s - loss: 0.4806 - acc: 0.7616\n",
      "Epoch 10/60\n",
      " - 2s - loss: 0.4805 - acc: 0.7604\n",
      "Epoch 11/60\n",
      " - 2s - loss: 0.4797 - acc: 0.7642\n",
      "Epoch 12/60\n",
      " - 2s - loss: 0.4789 - acc: 0.7629\n",
      "Epoch 13/60\n",
      " - 2s - loss: 0.4778 - acc: 0.7628\n",
      "Epoch 14/60\n",
      " - 2s - loss: 0.4784 - acc: 0.7623\n",
      "Epoch 15/60\n",
      " - 2s - loss: 0.4777 - acc: 0.7638\n",
      "Epoch 16/60\n",
      " - 2s - loss: 0.4772 - acc: 0.7632\n",
      "Epoch 17/60\n",
      " - 2s - loss: 0.4764 - acc: 0.7635\n",
      "Epoch 18/60\n",
      " - 2s - loss: 0.4772 - acc: 0.7618\n",
      "Epoch 19/60\n",
      " - 2s - loss: 0.4755 - acc: 0.7638\n",
      "Epoch 20/60\n",
      " - 2s - loss: 0.4770 - acc: 0.7668\n",
      "Epoch 21/60\n",
      " - 2s - loss: 0.4749 - acc: 0.7660\n",
      "Epoch 22/60\n",
      " - 2s - loss: 0.4768 - acc: 0.7651\n",
      "Epoch 23/60\n",
      " - 2s - loss: 0.4760 - acc: 0.7641\n",
      "Epoch 24/60\n",
      " - 2s - loss: 0.4755 - acc: 0.7652\n",
      "Epoch 25/60\n",
      " - 2s - loss: 0.4754 - acc: 0.7664\n",
      "Epoch 26/60\n",
      " - 2s - loss: 0.4748 - acc: 0.7647\n",
      "Epoch 27/60\n",
      " - 2s - loss: 0.4747 - acc: 0.7631\n",
      "Epoch 28/60\n",
      " - 2s - loss: 0.4750 - acc: 0.7660\n",
      "Epoch 29/60\n",
      " - 2s - loss: 0.4756 - acc: 0.7653\n",
      "Epoch 30/60\n",
      " - 2s - loss: 0.4743 - acc: 0.7656\n",
      "Epoch 31/60\n",
      " - 2s - loss: 0.4751 - acc: 0.7660\n",
      "Epoch 32/60\n",
      " - 2s - loss: 0.4748 - acc: 0.7645\n",
      "Epoch 33/60\n",
      " - 2s - loss: 0.4748 - acc: 0.7669\n",
      "Epoch 34/60\n",
      " - 2s - loss: 0.4745 - acc: 0.7661\n",
      "Epoch 35/60\n",
      " - 2s - loss: 0.4739 - acc: 0.7669\n",
      "Epoch 36/60\n",
      " - 2s - loss: 0.4749 - acc: 0.7658\n",
      "Epoch 37/60\n",
      " - 2s - loss: 0.4734 - acc: 0.7664\n",
      "Epoch 38/60\n",
      " - 2s - loss: 0.4735 - acc: 0.7663\n",
      "Epoch 39/60\n",
      " - 2s - loss: 0.4739 - acc: 0.7666\n",
      "Epoch 40/60\n",
      " - 2s - loss: 0.4740 - acc: 0.7653\n",
      "Epoch 41/60\n",
      " - 2s - loss: 0.4734 - acc: 0.7671\n",
      "Epoch 42/60\n",
      " - 2s - loss: 0.4727 - acc: 0.7655\n",
      "Epoch 43/60\n",
      " - 2s - loss: 0.4734 - acc: 0.7660\n",
      "Epoch 44/60\n",
      " - 2s - loss: 0.4733 - acc: 0.7658\n",
      "Epoch 45/60\n",
      " - 2s - loss: 0.4735 - acc: 0.7664\n",
      "Epoch 46/60\n",
      " - 2s - loss: 0.4733 - acc: 0.7678\n",
      "Epoch 47/60\n",
      " - 2s - loss: 0.4730 - acc: 0.7684\n",
      "Epoch 48/60\n",
      " - 2s - loss: 0.4731 - acc: 0.7691\n",
      "Epoch 49/60\n",
      " - 2s - loss: 0.4732 - acc: 0.7693\n",
      "Epoch 50/60\n",
      " - 2s - loss: 0.4730 - acc: 0.7669\n",
      "Epoch 51/60\n",
      " - 2s - loss: 0.4728 - acc: 0.7668\n",
      "Epoch 52/60\n",
      " - 2s - loss: 0.4717 - acc: 0.7681\n",
      "Epoch 53/60\n",
      " - 2s - loss: 0.4734 - acc: 0.7670\n",
      "Epoch 54/60\n",
      " - 2s - loss: 0.4733 - acc: 0.7669\n",
      "Epoch 55/60\n",
      " - 2s - loss: 0.4721 - acc: 0.7651\n",
      "Epoch 56/60\n",
      " - 2s - loss: 0.4741 - acc: 0.7660\n",
      "Epoch 57/60\n",
      " - 2s - loss: 0.4729 - acc: 0.7660\n",
      "Epoch 58/60\n",
      " - 2s - loss: 0.4719 - acc: 0.7663\n",
      "Epoch 59/60\n",
      " - 2s - loss: 0.4731 - acc: 0.7689\n",
      "Epoch 60/60\n",
      " - 2s - loss: 0.4722 - acc: 0.7661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed4f9ce710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "from keras import backend as K\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=10, epochs=60, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 70.779838\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
